#!/usr/bin/env python3
"""
New Pinecone Upload Script

This script performs two main functions:
1. Upload and embed new content chunks scraped from a client's website (generated by the crawler)
2. Delete any previously upserted web-scraped chunks so that only the current day's data remains.

All web-scraped records are stored in a dedicated namespace (e.g. "web_crawl_{CUSTOMER_ID}").
On the Serverless/Starter plan, metadata-based deletion isn't supported so we delete
all vectors in that namespace and then upsert the new data.

Usage:
    python upload_to_pinecone_new.py [--dry-run] [--input-dir DIRECTORY]
"""

import os
import sys
import glob
import json
import argparse
import logging
import uuid
import time
from datetime import datetime
from typing import List, Dict, Any, Tuple
import re
from dotenv import load_dotenv
from pinecone import Pinecone  # Pinecone SDK v3
from pathlib import Path
import unicodedata

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def read_markdown_and_metadata(file_path: str) -> Tuple[Dict[str, Any], str]:
    """
    Reads the markdown file content and the companion JSON metadata if available.

    Args:
        file_path: Path to the markdown file.

    Returns:
        A tuple (metadata, content) where metadata is a dictionary (empty if missing)
        and content is the markdown text.
    """
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # Expect companion metadata file with same name, replacing ".md" with "_metadata.json"
    metadata_path = re.sub(r"\.md$", "_metadata.json", file_path)
    metadata = {}
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
            logger.info(f"Loaded metadata from {metadata_path}")
        except Exception as e:
            logger.warning(f"Could not load metadata from {metadata_path}: {e}")
    return metadata, content


class PineconeUploader:
    """
    Handles connecting to Pinecone, uploading new records using integrated inference,
    and deleting outdated web-scraped records by wiping the dedicated namespace.
    """

    def __init__(
        self,
        api_key: str,
        environment: str,
        index_name: str,
        customer_id: str,
        index_host: str = None,
    ):
        """
        Initializes the uploader and connects to the correct Pinecone index.

        Args:
            api_key: Pinecone API key.
            environment: Pinecone environment (e.g. "us-east-1").
            index_name: The name of the Pinecone index.
            customer_id: The client identifier.
            index_host: (Optional) Pre-configured index host for faster access.
        """
        self.api_key = api_key
        self.environment = environment
        self.index_name = index_name
        self.customer_id = customer_id
        # Create a dedicated namespace for web-scraped content.
        self.web_namespace = f"web_crawl_{self.customer_id}"
        # Generate a new unique batch ID for this upload session.
        self.batch_id = f"batch_{uuid.uuid4().hex}"

        self.index_host = index_host

        # Initialize the Pinecone client using the modern SDK
        self.pc = Pinecone(api_key=self.api_key)

        # Determine and connect to the appropriate index host
        if self.index_host:
            logger.info(f"Using provided index host: {self.index_host}")
            self.index = self.pc.Index(name=self.index_name, host=self.index_host)
        else:
            try:
                index_info = self.pc.describe_index(self.index_name)
                self.index_host = index_info.host
                logger.info(f"Retrieved index host: {self.index_host}")
                self.index = self.pc.Index(name=self.index_name, host=self.index_host)
            except Exception as e:
                logger.error(f"Failed to retrieve index info: {e}")
                raise

    def sanitize_vector_id(self, id_str: str) -> str:
        """
        Sanitize vector ID to ensure it contains only ASCII characters.
        """
        normalized = unicodedata.normalize("NFKD", id_str)
        ascii_str = normalized.encode("ASCII", "ignore").decode("ASCII")
        sanitized = re.sub(r"[^a-zA-Z0-9_-]", "_", ascii_str)
        return sanitized

    def build_vector_records(
        self, records: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Build vector records for Pinecone upsert.
        Since integrated inference is not available, we'll store text in metadata
        and use placeholder vectors until we implement proper embedding.
        """
        vector_records = []
        seen_ids = set()
        for record in records:
            metadata = record["metadata"]
            # Include chunk_index in the vector ID to ensure uniqueness
            base_id = self.sanitize_vector_id(metadata["url"])
            vector_id = f"{base_id}_chunk_{metadata['chunk_index']}"
            if vector_id in seen_ids:
                logger.warning(f"Duplicate vector ID found: {vector_id}")
            else:
                seen_ids.add(vector_id)
            vector_record = {
                "id": vector_id,
                "values": [0.1] * 1024,  # Placeholder vector
                "metadata": {
                    # Source information
                    "url": metadata["url"],
                    "title": metadata["title"],
                    "source_domain": metadata.get("source_domain", ""),
                    "source_path": metadata.get("source_path", ""),
                    # Content information
                    "text": record["chunk_text"],
                    "keywords": metadata.get("keywords", []),
                    "token_count": metadata.get("token_count", 0),
                    # Chunk information
                    "chunk_id": metadata["chunk_id"],
                    "chunk_index": metadata["chunk_index"],
                    "total_chunks": metadata["total_chunks"],
                    "chunk_name": metadata.get("chunk_name", ""),
                    # Tracking information
                    "crawl_timestamp": metadata.get("crawl_timestamp", ""),
                    "upload_timestamp": metadata["upload_timestamp"],
                    "batch_id": metadata["batch_id"],
                    "source_type": metadata["source_type"],
                    # Customer information
                    "customer_id": metadata.get("customer_id", ""),
                },
            }
            vector_records.append(vector_record)
            logger.debug(f"Created vector record with ID: {vector_id}")
        logger.info(
            f"Created {len(vector_records)} vector records, {len(seen_ids)} unique IDs"
        )
        return vector_records

    def prepare_records(self, chunks_dir: str) -> List[Dict[str, Any]]:
        """
        Scans the provided directory for markdown chunk files and builds a list of records.
        Each record is tagged with the current batch_id and upload timestamp.
        """
        record_list = []
        md_files = glob.glob(os.path.join(chunks_dir, "*.md"))
        if not md_files:
            logger.warning(f"No markdown files found in directory: {chunks_dir}")
            return []
        current_time = datetime.now().isoformat()
        for md_file in md_files:
            try:
                metadata, content = read_markdown_and_metadata(md_file)
                if "chunk_id" not in metadata:
                    metadata["chunk_id"] = os.path.splitext(os.path.basename(md_file))[
                        0
                    ]
                metadata["batch_id"] = self.batch_id
                metadata["upload_timestamp"] = current_time
                metadata["source_type"] = "web_crawl"
                record = {
                    "id": metadata["chunk_id"],
                    "chunk_text": content,
                    "metadata": metadata,
                }
                record_list.append(record)
                logger.info(f"Prepared record for file: {md_file}")
            except Exception as e:
                logger.error(f"Error processing file {md_file}: {e}")
                continue
        return record_list

    def upsert_records(self, records: List[Dict[str, Any]]) -> int:
        """
        Upserts the provided records to the Pinecone index within the dedicated web namespace.
        Uses integrated inference by sending text in the "text" field.
        """
        try:
            logger.info(
                f"Upserting {len(records)} records into namespace: {self.web_namespace}"
            )
            vectors = self.build_vector_records(records)
            # Use the standard upsert method
            self.index.upsert(vectors=vectors, namespace=self.web_namespace)
            logger.info("Upsert operation completed successfully")
            return len(records)
        except Exception as e:
            logger.error(f"Error during upsert: {e}")
            raise

    def delete_old_records(self) -> int:
        """
        Deletes all vectors from the dedicated web namespace.
        On Serverless/Starter plans, metadata-based deletion is not supported.
        Instead, we delete all vectors in the dedicated namespace.
        """
        try:
            before_stats = self.index.describe_index_stats()
            before_count = (
                before_stats.get("namespaces", {})
                .get(self.web_namespace, {})
                .get("vector_count", 0)
            )
            logger.info(f"Deleting all records in namespace '{self.web_namespace}'")
            self.index.delete(delete_all=True, namespace=self.web_namespace)
            time.sleep(2)  # Allow time for deletion consistency.
            after_stats = self.index.describe_index_stats()
            after_count = (
                after_stats.get("namespaces", {})
                .get(self.web_namespace, {})
                .get("vector_count", 0)
            )
            deleted_count = before_count - after_count
            logger.info(
                f"Deleted {deleted_count} records in namespace '{self.web_namespace}'"
            )
            return deleted_count
        except Exception as e:
            if "Namespace not found" in str(e):
                logger.info(
                    f"Namespace '{self.web_namespace}' does not exist yet - this is expected for first run"
                )
                return 0
            logger.error(f"Error during record deletion: {e}")
            raise


def main():
    parser = argparse.ArgumentParser(
        description="Upload and update Pinecone records from web-crawled content chunks."
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Process and prepare records, but do not perform any upsert or delete operations",
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        default="crawler/winery_content",
        help="Directory containing the web crawl content (expects a 'chunks' subdirectory)",
    )
    args = parser.parse_args()

    load_dotenv()
    # Validate required environment variables.
    required_vars = ["PINECONE_API_KEY", "CUSTOMER_ID", "PINECONE_INDEX_NAME"]
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        logger.error(f"Missing environment variables: {', '.join(missing)}")
        sys.exit(1)

    api_key = os.getenv("PINECONE_API_KEY")
    customer_id = os.getenv("CUSTOMER_ID")
    environment = os.getenv("PINECONE_ENVIRONMENT", "us-east-1")
    index_name = os.getenv("PINECONE_INDEX_NAME")
    index_host = os.getenv("PINECONE_HOST")  # optional

    chunks_dir = os.path.join(args.input_dir, "chunks")
    if not os.path.exists(chunks_dir):
        logger.error(f"Chunks directory not found: {chunks_dir}")
        sys.exit(1)

    # Initialize the PineconeUploader instance.
    uploader = PineconeUploader(
        api_key, environment, index_name, customer_id, index_host
    )
    records = uploader.prepare_records(chunks_dir)
    if not records:
        logger.error("No records to process; exiting.")
        sys.exit(1)

    if args.dry_run:
        logger.info(
            "Dry run mode active: records prepared but not uploaded or deleted."
        )
        sys.exit(0)

    # Delete old records by wiping out the dedicated namespace.
    deleted_count = uploader.delete_old_records()
    # Upsert new records.
    upserted_count = uploader.upsert_records(records)

    logger.info(
        f"Upload complete: {upserted_count} records upserted; {deleted_count} old records deleted."
    )


if __name__ == "__main__":
    main()
