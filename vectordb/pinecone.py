#!/usr/bin/env python3
"""
New Pinecone Upload Script

This script performs two main functions:
1. Upload and embed new content chunks scraped from a client's website
   (generated by the crawler)
2. Delete any previously upserted web-scraped chunks so that only the
   current day's data remains.

All web-scraped records are stored in a dedicated namespace
(e.g. "web_crawl_{CUSTOMER_ID}").
On the Serverless/Starter plan, metadata-based deletion isn't supported
so we delete all vectors in that namespace and then upsert the new data.

Usage:
    python upload_to_pinecone_new.py [--dry-run] [--input-dir DIRECTORY]
"""

import os
import sys
import glob
import logging
import uuid
import time
from datetime import datetime
from typing import List, Dict, Any, Tuple
import re
from dotenv import load_dotenv
from pinecone import Pinecone  # Updated import for Pinecone SDK v6+
import unicodedata

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

CHUNK_ID_PREFIX = "web_crawl"


class PineconeUploader:
    """
    Handles connecting to Pinecone, uploading new records using integrated inference,
    and deleting outdated web-scraped records by wiping the dedicated namespace.
    """

    def __init__(
        self,
        api_key: str,
        index_name: str,
    ):
        """
        Initializes the uploader and connects to the correct Pinecone index.
        """
        self.api_key = api_key
        self.index_name = index_name
        self.web_namespace = ""  # Use default namespace

        # Initialize the Pinecone client
        self.pc = Pinecone(api_key=self.api_key)

        # Add diagnostic logging
        logger.info(f"Initializing Pinecone client with index: {index_name}")

        # Connect to the index
        try:
            self.index = self.pc.Index(self.index_name)
            logger.info(f"Connected to index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to connect to index: {e}")
            raise

        # Verify index connection
        try:
            stats = self.index.describe_index_stats()
            logger.info(f"Index stats: {stats}")
        except Exception as e:
            logger.error(f"Failed to get index stats: {e}")

    def sanitize_vector_id(self, id_str: str) -> str:
        """
        Sanitize vector ID to ensure it contains only ASCII characters.
        """
        normalized = unicodedata.normalize("NFKD", id_str)
        ascii_str = normalized.encode("ASCII", "ignore").decode("ASCII")
        sanitized = re.sub(r"[^a-zA-Z0-9_-]", "_", ascii_str)
        return sanitized

    def upsert_records(self, records: List[Dict[str, Any]]) -> int:
        """
        Upserts the provided records to the Pinecone index using server-side embedding.
        Uses the upsert_records method which handles text-to-vector conversion on Pinecone's side.
        """
        try:
            logger.info(
                f"Upserting {len(records)} records into namespace: {self.web_namespace}"
            )

            # Format records for upsert_records
            formatted_records = []
            for record in records:
                metadata = record["metadata"]
                # Create a record with ID and text field for embedding
                formatted_record = {
                    # Provide ID directly as a field
                    "_id": self.sanitize_vector_id(
                        f"{CHUNK_ID_PREFIX}_{metadata["chunk_name"]}"
                    ),
                    # Text to be embedded
                    "chunk_text": record["chunk_text"],
                    # Additional metadata
                    "url": metadata["url"],
                    "upload_timestamp": metadata["upload_timestamp"],
                }
                formatted_records.append(formatted_record)

            # Use upsert_records which handles server-side embedding
            self.index.upsert_records(
                namespace=self.web_namespace, records=formatted_records
            )

            logger.info("Upsert operation completed successfully")
            return len(records)
        except Exception as e:
            logger.error(f"Error during upsert: {e}")
            raise

    def delete_records_by_date_prefix(self, date_str=None) -> int:
        """
        Deletes vectors with ID prefixes matching a specific date.
        If no date is provided, deletes yesterday's vectors.
        This allows for precise deletion of old web crawl data without affecting
        other data in the default namespace.

        Args:
            date_str: Optional date string in YYYYMMDD format. If not provided,
                    defaults to yesterday's date.

        Returns:
            The number of deleted records.
        """
        try:
            # If no date provided, use yesterday's date
            if not date_str:
                from datetime import datetime, timedelta

                yesterday = datetime.now() - timedelta(days=1)
                date_str = yesterday.strftime("%Y%m%d")

            # Create the prefix pattern to match
            prefix = f"web_crawl_{date_str}"
            logger.info(f"Listing vectors with prefix '{prefix}' for deletion")

            # Get all vector IDs matching the prefix
            total_deleted = 0
            deleted_batch = 0

            # Paginate through all vectors with this prefix
            for ids_batch in self.index.list(
                prefix=prefix, namespace=self.web_namespace
            ):
                if ids_batch:
                    logger.info(f"Deleting batch of {len(ids_batch)} vectors")
                    self.index.delete(ids=ids_batch, namespace=self.web_namespace)
                    deleted_batch += len(ids_batch)
                    total_deleted += len(ids_batch)

                    # Log progress for large deletions
                    if deleted_batch >= 10000:
                        logger.info(f"Deleted {total_deleted} vectors so far...")
                        deleted_batch = 0

                    # Small pause to avoid overwhelming the API
                    time.sleep(0.1)

            logger.info(
                f"Successfully deleted {total_deleted} vectors with prefix '{prefix}'"
            )
            return total_deleted

        except Exception as e:
            logger.error(
                f"Error during targeted deletion of vectors with prefix '{date_str}': {e}"
            )
            raise

    def delete_old_web_crawl_records(self, keep_most_recent=1) -> int:
        """
        Find and delete older web crawl data, keeping only the most recent crawls.
        """
        try:
            # Add a small delay to ensure Pinecone has processed the recent upsert
            time.sleep(2)

            # First check if we have any vectors at all
            stats = self.index.describe_index_stats()
            total_vectors = stats["total_vector_count"]
            logger.info(f"Total vectors in index: {total_vectors}")

            if total_vectors == 0:
                logger.info("Index is empty, nothing to delete")
                return 0

            # Get all vectors with web_crawl_ prefix
            all_vectors = []
            logger.info("Listing all web crawl vectors...")
            current_prefix = f"web_crawl_{self.batch_id}"

            # First get vectors NOT matching current batch (these will be deleted)
            for batch in self.index.list(
                prefix="web_crawl_", namespace=self.web_namespace
            ):
                logger.info(f"Found batch of {len(batch)} vectors")
                # Filter out vectors from current batch
                old_vectors = [v for v in batch if not v.startswith(current_prefix)]
                if old_vectors:
                    logger.info(
                        f"Found {len(old_vectors)} vectors from previous batches"
                    )
                    all_vectors.extend(old_vectors)
                else:
                    logger.info("No old vectors found in this batch")

            if not all_vectors:
                logger.info("No old vectors found to delete")
                return 0

            # Delete old vectors in batches
            total_deleted = 0
            batch_size = 100
            for i in range(0, len(all_vectors), batch_size):
                batch = all_vectors[i : i + batch_size]
                logger.info(f"Deleting batch of {len(batch)} vectors: {batch[:5]}...")
                self.index.delete(ids=batch, namespace=self.web_namespace)
                total_deleted += len(batch)
                logger.info(f"Deleted batch of {len(batch)} old vectors")
                time.sleep(0.1)  # Small pause between batches

            logger.info(
                f"Successfully deleted {total_deleted} vectors from old batches"
            )
            return total_deleted

        except Exception as e:
            logger.error(f"Error during deletion of old web crawl records: {e}")
            raise


def main(chunks):
    load_dotenv()
    # Validate required environment variables.
    required_vars = ["PINECONE_API_KEY", "PINECONE_INDEX_NAME"]
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        logger.error(f"Missing environment variables: {', '.join(missing)}")
        sys.exit(1)

    api_key = os.getenv("PINECONE_API_KEY")
    index_name = os.getenv("PINECONE_INDEX_NAME")

    # Initialize the PineconeUploader instance.
    uploader = PineconeUploader(api_key, index_name)
    records = uploader.prepare_records(chunks_dir)
    if not records:
        logger.error("No records to process; exiting.")
        sys.exit(1)

    if args.dry_run:
        logger.info(
            "Dry run mode active: records prepared but not uploaded or deleted."
        )
        sys.exit(0)

    # Upsert new records.
    upserted_count = uploader.upsert_records(records)

    # Handle deletion based on command line argument
    if args.delete_date.lower() == "none":
        logger.info("Skipping deletion of old records as requested")
        deleted_count = 0
    else:
        deleted_count = uploader.delete_old_web_crawl_records(args.keep_recent)

    logger.info(
        f"Upload complete: {upserted_count} records upserted, {deleted_count} old records deleted."
    )


if __name__ == "__main__":
    main()
