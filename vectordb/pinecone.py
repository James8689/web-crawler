#!/usr/bin/env python3
"""
New Pinecone Upload Script

This script performs two main functions:
1. Upload and embed new content chunks scraped from a client's website
   (generated by the crawler)
2. Delete any previously upserted web-scraped chunks so that only the
   current day's data remains.

All web-scraped records are stored in a dedicated namespace
(e.g. "web_crawl_{CUSTOMER_ID}").
On the Serverless/Starter plan, metadata-based deletion isn't supported
so we delete all vectors in that namespace and then upsert the new data.

Usage:
    python upload_to_pinecone_new.py [--dry-run] [--input-dir DIRECTORY]
"""

import os
import sys
import logging
import time
from datetime import datetime
import re
from dotenv import load_dotenv

# Import from the pinecone.control module where the Pinecone class is defined
from pinecone import Pinecone
import unicodedata

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

CHUNK_ID_PREFIX = "web_crawl"


class PineconeUploader:
    """
    Handles connecting to Pinecone, uploading new records using integrated
    inference, and deleting outdated web-scraped records by wiping the
    dedicated namespace.
    """

    def __init__(
        self,
        api_key: str,
        index_name: str,
    ):
        """
        Initializes the uploader and connects to the correct Pinecone index.
        """
        self.api_key = api_key
        self.index_name = index_name
        self.web_namespace = ""  # Use default namespace
        # Generate a batch ID using timestamp for tracking current batch
        self.batch_id = datetime.now().strftime("%Y%m%d%H%M%S")

        # Initialize the Pinecone client
        self.pc = Pinecone(api_key=self.api_key)

        # Add diagnostic logging
        logger.info(f"Initializing Pinecone client with index: {index_name}")

        # Connect to the index
        try:
            self.index = self.pc.Index(self.index_name)
            logger.info(f"Connected to index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to connect to index: {e}")
            raise

        # Verify index connection
        try:
            stats = self.index.describe_index_stats()
            logger.info(f"Index stats: {stats}")
        except Exception as e:
            logger.error(f"Failed to get index stats: {e}")

    def sanitize_vector_id(self, id_str: str) -> str:
        """
        Sanitize vector ID to ensure it contains only ASCII characters.
        """
        normalized = unicodedata.normalize("NFKD", id_str)
        ascii_str = normalized.encode("ASCII", "ignore").decode("ASCII")
        sanitized = re.sub(r"[^a-zA-Z0-9_-]", "_", ascii_str)
        return sanitized

    def upsert_records(self, records) -> int:
        """
        Upserts the provided records to the Pinecone index using server-side
        embedding. Uses the upsert_records method which handles text-to-vector
        conversion on Pinecone's side.

        Records are processed in batches of 50 to prevent timeouts and memory issues.
        """
        try:
            total_records = len(records)
            logger.info(
                f"Upserting {total_records} records into namespace: {self.web_namespace}"
            )

            # Format all records first
            formatted_records = []
            for record in records:
                # Access attributes directly since record is a namespace object
                url = record.url
                chunk_name = record.chunk_name
                chunk_text = record.markdown

                # Create a record with ID and text field for embedding
                formatted_record = {
                    # Provide ID directly as a field
                    "_id": self.sanitize_vector_id(f"{CHUNK_ID_PREFIX}_{chunk_name}"),
                    # Text to be embedded
                    "chunk_text": chunk_text,
                    # Additional metadata
                    "url": url,
                    "upload_timestamp": datetime.now().isoformat(),
                }
                formatted_records.append(formatted_record)

            # Process records in batches of 50
            batch_size = 50
            total_batches = (len(formatted_records) + batch_size - 1) // batch_size
            records_upserted = 0

            logger.info(
                f"Processing {total_batches} batches of up to {batch_size} records each"
            )

            for i in range(total_batches):
                start_idx = i * batch_size
                end_idx = min(start_idx + batch_size, len(formatted_records))
                batch = formatted_records[start_idx:end_idx]

                batch_count = len(batch)
                logger.info(
                    f"Upserting batch {i+1}/{total_batches} with {batch_count} records"
                )

                # Use upsert_records which handles server-side embedding
                self.index.upsert_records(self.web_namespace, batch)
                records_upserted += batch_count

                logger.info(
                    f"Completed batch {i+1}/{total_batches}. Progress: {records_upserted}/{total_records}"
                )

                # Small pause between batches to avoid rate limiting
                if i < total_batches - 1:
                    time.sleep(0.5)

            logger.info(
                f"Upsert operation completed successfully. Total records upserted: {records_upserted}"
            )
            return records_upserted
        except Exception as e:
            logger.error(f"Error during upsert: {e}")
            raise

    def delete_older_than_one_hour(self) -> int:
        """
        Delete all records where:
        1. The ID starts with CHUNK_ID_PREFIX
        2. They are older than one hour based on timestamp metadata

        Returns:
            The number of deleted records.
        """
        try:
            logger.info("Starting deletion of records older than one hour")

            # Calculate timestamp for one hour ago
            from datetime import datetime, timedelta

            one_hour_ago = datetime.now() - timedelta(hours=1)
            logger.info(f"Threshold time: {one_hour_ago.isoformat()}")

            # Get all vectors with the CHUNK_ID_PREFIX
            all_old_vectors = []
            logger.info(f"Listing all vectors with prefix '{CHUNK_ID_PREFIX}'...")

            # Get all vector IDs with CHUNK_ID_PREFIX first
            all_matching_ids = []
            for batch in self.index.list(
                prefix=CHUNK_ID_PREFIX, namespace=self.web_namespace
            ):
                if batch:
                    all_matching_ids.extend(batch)
                    logger.info(f"Found batch of {len(batch)} vectors with prefix")
                    # Small pause between batch processing
                    time.sleep(0.1)

            if not all_matching_ids:
                logger.info(f"No vectors with prefix '{CHUNK_ID_PREFIX}' found")
                return 0

            logger.info(f"Total {len(all_matching_ids)} vectors with matching prefix")

            # Fetch all vectors at once to check timestamps
            try:
                logger.info(
                    f"Fetching all {len(all_matching_ids)} vectors to check timestamps"
                )
                # Fetch vectors using the Pinecone v6 format
                response = self.index.fetch(
                    ids=all_matching_ids, namespace=self.web_namespace
                )

                # Access the vectors attribute from the response object
                # In Pinecone v6, response is an object with a 'vectors' attribute
                vectors_dict = response.vectors

                # Process all vectors
                for vector_id, vector_data in vectors_dict.items():
                    upload_timestamp = vector_data.metadata["upload_timestamp"]

                    if upload_timestamp:
                        # Parse timestamp and compare
                        try:
                            record_timestamp = datetime.fromisoformat(upload_timestamp)
                            if record_timestamp < one_hour_ago:
                                all_old_vectors.append(vector_id)
                                logger.debug(
                                    f"Vector {vector_id} is older than one hour "
                                    f"(uploaded at {upload_timestamp})"
                                )
                        except (ValueError, TypeError) as e:
                            logger.warning(
                                f"Could not parse timestamp {upload_timestamp} "
                                f"for vector {vector_id}: {e}"
                            )
                    else:
                        logger.warning(
                            f"Vector {vector_id} has no upload_timestamp metadata"
                        )

            except Exception as e:
                logger.error(f"Error fetching vectors: {e}")
                import traceback

                traceback.print_exc()
                return 0

            # Check if we found any old vectors
            if not all_old_vectors:
                logger.info("No vectors older than one hour found")
                return 0

            logger.info(f"Found {len(all_old_vectors)} vectors older than one hour")

            # Delete all old vectors at once
            try:
                logger.info(f"Deleting all {len(all_old_vectors)} old vectors at once")
                self.index.delete(ids=all_old_vectors, namespace=self.web_namespace)
                total_deleted = len(all_old_vectors)
                logger.info(
                    f"Successfully deleted {total_deleted} vectors older than one hour"
                )
                return total_deleted
            except Exception as e:
                logger.error(f"Error deleting vectors: {e}")
                return 0

        except Exception as e:
            logger.error(f"Error during deletion of old records: {e}")
            raise


def upload_chunks(chunks):
    load_dotenv()
    # Validate required environment variables.
    required_vars = ["PINECONE_API_KEY", "PINECONE_INDEX_NAME"]
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        logger.error(f"Missing environment variables: {', '.join(missing)}")
        sys.exit(1)

    api_key = os.getenv("PINECONE_API_KEY")
    index_name = os.getenv("PINECONE_INDEX_NAME")

    # Initialize the PineconeUploader instance.
    uploader = PineconeUploader(api_key, index_name)

    upserted_count = uploader.upsert_records(chunks)

    logger.info(f"Upserted {upserted_count} records")

    deleted_count = uploader.delete_older_than_one_hour()

    logger.info(f"Deleted {deleted_count} old records")

    logger.info(
        f"Operation complete: {upserted_count} records upserted, {deleted_count} old records deleted."
    )
