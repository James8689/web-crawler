#!/usr/bin/env python3
"""
Standalone Pinecone Upload Script

This script takes markdown files generated by a web scraper and uploads them to Pinecone.
It's designed to be run as a separate step after the web scraping process and works with
the enhanced chunking functionality that generates metadata JSON files.

Usage:
    python upload_to_pinecone.py [--dry] [--input-dir DIRECTORY]

Options:
    --dry           Run in dry run mode (process but don't upload to Pinecone)
    --input-dir     Directory containing markdown files (default: winery_content)
"""

import os
import sys
import glob
import json
import argparse
import logging
import uuid
import time
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import re
from dotenv import load_dotenv
from pinecone import Pinecone

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PineconeUploader:
    """Handles uploading content to Pinecone."""
    
    def __init__(self, api_key: str, environment: str = "us-east-1", 
                 index_name: str = None, namespace: str = "default",
                 embedding_model: str = "llama-text-embed-v2"):
        """
        Initialize the PineconeUploader.
        
        Args:
            api_key: Pinecone API key
            environment: Pinecone environment (e.g., us-east-1)
            index_name: Name of the Pinecone index
            namespace: Namespace for the vectors
            embedding_model: Name of the embedding model
        """
        self.api_key = api_key
        self.environment = environment
        self.index_name = index_name
        self.namespace = namespace
        self.embedding_model = embedding_model
        
        # Initialize Pinecone client
        self.pc = Pinecone(api_key=self.api_key)
        
        # Ensure index exists
        self._ensure_index_exists()
    
    def _ensure_index_exists(self) -> None:
        """Ensure that the Pinecone index exists, creating it if necessary."""
        # Check if index exists
        if not self.pc.has_index(self.index_name):
            logger.info(f"Creating new index: {self.index_name}")
            
            # Create index with integrated embedding model
            self.pc.create_index_for_model(
                name=self.index_name,
                cloud="aws",
                region=self.environment,
                embed={
                    "model": self.embedding_model,
                    "field_map": {"text": "chunk_text"}
                }
            )
            logger.info(f"Index {self.index_name} created successfully")
        else:
            logger.info(f"Using existing index: {self.index_name}")
    
    def upsert_records(self, records: List[Dict[str, Any]]) -> int:
        """
        Upsert records to the Pinecone index.
        
        Args:
            records: List of records to upsert
            
        Returns:
            Number of records upserted
        """
        # Get the index
        index = self.pc.index(self.index_name)
        
        # Upsert records
        logger.info(f"Upserting {len(records)} records to namespace '{self.namespace}'")
        index.upsert_records(records=records, namespace=self.namespace)
        
        # Wait a moment for consistency
        time.sleep(2)
        
        # Get index stats
        stats = index.describe_index_stats()
        logger.info(f"Index stats after upsert: {stats}")
        
        return len(records)

def read_markdown_file(file_path: str) -> str:
    """
    Read content from a markdown file.
    
    Args:
        file_path: Path to the markdown file
        
    Returns:
        Content of the markdown file
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def load_json_metadata(file_path: str) -> Dict[str, Any]:
    """
    Load metadata from a JSON file.
    
    Args:
        file_path: Path to the JSON metadata file
        
    Returns:
        Dictionary containing metadata
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"Failed to load metadata from {file_path}: {e}")
        return {}

def get_metadata_file_path(markdown_file_path: str) -> str:
    """
    Generate the expected path to the metadata JSON file for a given markdown file.
    
    Args:
        markdown_file_path: Path to the markdown file
        
    Returns:
        Path to the expected metadata JSON file
    """
    base_path = os.path.splitext(markdown_file_path)[0]
    return f"{base_path}_metadata.json"

def check_for_chunk_index(input_dir: str) -> Optional[Dict[str, Any]]:
    """
    Check for and load the chunk index file.
    
    Args:
        input_dir: Directory containing the chunk index
        
    Returns:
        Dictionary containing the chunk index if found, None otherwise
    """
    index_path = os.path.join(input_dir, "chunk_index.json")
    if os.path.exists(index_path):
        try:
            with open(index_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load chunk index from {index_path}: {e}")
    return None

def extract_metadata_from_file(file_path: str, customer_id: str) -> Tuple[Dict[str, Any], str]:
    """
    Extract metadata and content from a file, using companion JSON metadata if available.
    
    Args:
        file_path: Path to the markdown file
        customer_id: Customer identifier
        
    Returns:
        Tuple containing (metadata dictionary, content)
    """
    # Read content
    content = read_markdown_file(file_path)
    
    # Look for companion metadata file
    metadata_file = get_metadata_file_path(file_path)
    metadata = {}
    
    if os.path.exists(metadata_file):
        # Use the companion metadata file
        metadata = load_json_metadata(metadata_file)
        logger.debug(f"Loaded metadata from {metadata_file}")
    else:
        # Extract information from filename and content
        filename = os.path.basename(file_path)
        name_without_ext = os.path.splitext(filename)[0]
        
        # Try to extract title from content
        title_pattern = re.compile(r'^#\s+(.+)$', re.MULTILINE)
        title_match = title_pattern.search(content)
        
        if title_match:
            title = title_match.group(1).strip()
        else:
            title = name_without_ext.replace('_', ' ').title()
        
        # Basic metadata
        metadata = {
            "source": name_without_ext,
            "title": title,
        }
        
        # Check if this is a chunk file
        chunk_match = re.search(r'_(\d+)_([^_]+)$', name_without_ext)
        if chunk_match:
            chunk_index = int(chunk_match.group(1))
            keywords = chunk_match.group(2).split('_')
            
            metadata.update({
                "chunk_index": chunk_index,
                "keywords": keywords,
            })
    
    # Add common fields
    metadata.update({
        "file_path": file_path,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "customer_id": customer_id,
        # Generate a chunk_id if one doesn't exist
        "chunk_id": metadata.get("chunk_id", f"{os.path.basename(file_path)}_{uuid.uuid4().hex[:8]}")
    })
    
    return metadata, content

def save_chunks_to_disk(chunks: List[Dict[str, Any]], output_dir: str) -> List[str]:
    """
    Save chunks to disk for dry run mode.
    
    Args:
        chunks: List of dictionaries containing chunk text and metadata
        output_dir: Directory to save chunks
        
    Returns:
        List of saved file paths
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    saved_files = []
    for i, chunk in enumerate(chunks):
        # Generate filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chunk_id = chunk["metadata"].get("chunk_id", f"chunk_{i}")
        filename = f"{output_dir}/{timestamp}_{chunk_id}.json"
        
        # Prepare data to save
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                "chunk_text": chunk["chunk_text"],
                "metadata": chunk["metadata"]
            }, f, ensure_ascii=False, indent=2)
        
        saved_files.append(filename)
    
    logger.info(f"Saved {len(saved_files)} chunks to {output_dir}")
    return saved_files

def process_markdown_files(input_dir: str, dry_run: bool = False) -> int:
    """
    Process markdown files and upload to Pinecone.
    
    Args:
        input_dir: Directory containing markdown files
        dry_run: Whether to run in dry run mode
        
    Returns:
        Number of files processed
    """
    # Load environment variables
    load_dotenv()
    
    # Check required environment variables
    required_vars = ["PINECONE_API_KEY", "CUSTOMER_ID"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        logger.error(f"Missing required environment variables: {', '.join(missing_vars)}")
        return 0
    
    # Get environment variables
    api_key = os.getenv("PINECONE_API_KEY")
    customer_id = os.getenv("CUSTOMER_ID")
    environment = os.getenv("PINECONE_ENVIRONMENT", "us-east-1")
    index_name = os.getenv("PINECONE_INDEX_NAME", f"rag-{customer_id.lower()}")
    namespace = os.getenv("NAMESPACE", "default")
    embedding_model = os.getenv("EMBEDDING_MODEL", "llama-text-embed-v2")
    
    # Check if directory exists
    if not os.path.exists(input_dir):
        logger.error(f"Directory not found: {input_dir}")
        return 0
    
    # Check for chunk index file (created by our enhanced chunking process)
    chunk_index = check_for_chunk_index(input_dir)
    
    # Check for chunks directory
    chunks_dir = os.path.join(input_dir, "chunks")
    use_pre_chunked = os.path.exists(chunks_dir)
    
    if use_pre_chunked:
        logger.info(f"Found pre-chunked content in {chunks_dir}")
        
        if chunk_index:
            logger.info(f"Found chunk index with {len(chunk_index)} entries")
        
        # Get all chunk files
        chunk_files = glob.glob(os.path.join(chunks_dir, "*.md"))
        if not chunk_files:
            logger.warning(f"No chunk files found in {chunks_dir}")
            use_pre_chunked = False
    
    # If no pre-chunked content, use regular markdown files
    if not use_pre_chunked:
        logger.info(f"No pre-chunked content found, using regular markdown files from {input_dir}")
        markdown_files = glob.glob(os.path.join(input_dir, "*.md"))
        
        if not markdown_files:
            logger.warning(f"No markdown files found in {input_dir}")
            return 0
        
        logger.info(f"Found {len(markdown_files)} markdown files in {input_dir}")
    else:
        logger.info(f"Found {len(chunk_files)} pre-chunked files in {chunks_dir}")
    
    # Process files
    all_chunks = []
    
    if use_pre_chunked:
        # Process pre-chunked files
        for file_path in chunk_files:
            try:
                # Extract metadata and content using companion JSON if available
                metadata, content = extract_metadata_from_file(file_path, customer_id)
                
                # Create chunk record
                chunk = {
                    "chunk_text": content,
                    "metadata": metadata
                }
                all_chunks.append(chunk)
                
                logger.info(f"Processed pre-chunked file: {file_path}")
            except Exception as e:
                logger.error(f"Error processing {file_path}: {str(e)}")
                continue
    else:
        # Process regular markdown files (no chunking - each file becomes one document)
        markdown_files = glob.glob(os.path.join(input_dir, "*.md"))
        for file_path in markdown_files:
            try:
                # Extract metadata and content
                metadata, content = extract_metadata_from_file(file_path, customer_id)
                
                # Create single chunk for the entire file
                chunk = {
                    "chunk_text": content,
                    "metadata": metadata
                }
                all_chunks.append(chunk)
                
                logger.info(f"Processed full document: {file_path}")
            except Exception as e:
                logger.error(f"Error processing {file_path}: {str(e)}")
                continue
    
    # Handle chunks based on mode
    if dry_run:
        # Dry run mode: save to disk
        output_dir = os.getenv("OUTPUT_DIR", "processed_content")
        save_chunks_to_disk(all_chunks, output_dir)
    else:
        # Embedding mode: upload to Pinecone
        try:
            # Initialize Pinecone uploader
            uploader = PineconeUploader(
                api_key=api_key,
                environment=environment,
                index_name=index_name,
                namespace=namespace,
                embedding_model=embedding_model
            )
            
            # Convert chunks to Pinecone records format
            records = []
            for chunk in all_chunks:
                record = {
                    "_id": chunk["metadata"].get("chunk_id", str(uuid.uuid4())),
                    "chunk_text": chunk["chunk_text"]
                }
                # Add all metadata as fields
                for key, value in chunk["metadata"].items():
                    # Skip any overly complex metadata that can't be stored in Pinecone
                    if isinstance(value, (str, int, float, bool, list)) or value is None:
                        record[key] = value
                
                records.append(record)
            
            # Upsert to Pinecone
            uploader.upsert_records(records)
        except Exception as e:
            logger.error(f"Error uploading to Pinecone: {str(e)}")
            return 0
    
    return len(all_chunks)

def main():
    """Main entry point for the script."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Upload markdown files to Pinecone'
    )
    parser.add_argument(
        '--dry', 
        action='store_true', 
        help='Run in dry mode (process but don\'t upload)'
    )
    parser.add_argument(
        '--input-dir', 
        type=str, 
        default='winery_content',
        help='Directory containing markdown files'
    )
    args = parser.parse_args()
    
    try:
        # Process markdown files
        num_processed = process_markdown_files(
            input_dir=args.input_dir,
            dry_run=args.dry
        )
        
        if num_processed > 0:
            logger.info(f"Successfully processed {num_processed} chunks")
            return 0
        else:
            logger.error("No files were processed")
            return 1
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        return 1

if __name__ == "__main__":
    sys.exit(main())