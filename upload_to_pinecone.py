#!/usr/bin/env python3
"""
Standalone Pinecone Upload Script

This script takes markdown files generated by a web scraper and uploads them to Pinecone.
It's designed to be run as a separate step after the web scraping process.

Usage:
    python upload_to_pinecone.py [--dry] [--input-dir DIRECTORY]

Options:
    --dry           Run in dry run mode (process but don't upload to Pinecone)
    --input-dir     Directory containing markdown files (default: winery_content)
"""

import os
import sys
import glob
import argparse
import logging
import uuid
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
import re
from dotenv import load_dotenv
from pinecone import Pinecone

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ContentChunker:
    """Handles chunking of text content for optimal embedding."""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        Initialize the ContentChunker.
        
        Args:
            chunk_size: Target size of each chunk in characters
            chunk_overlap: Overlap between chunks in characters
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def chunk_text_by_sentences(self, text: str) -> List[str]:
        """
        Split text into chunks by sentences, respecting chunk size and overlap.
        
        Args:
            text: Text content to chunk
            
        Returns:
            List of text chunks
        """
        # Simple sentence splitting - can be enhanced with nltk if available
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            # If adding this sentence would exceed chunk size and we already have content,
            # complete the current chunk and start a new one
            if current_length + sentence_length > self.chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                
                # Keep some sentences for overlap
                overlap_sentences = []
                overlap_length = 0
                
                # Work backwards through current_chunk to build overlap
                for prev_sentence in reversed(current_chunk):
                    if overlap_length + len(prev_sentence) <= self.chunk_overlap:
                        overlap_sentences.insert(0, prev_sentence)
                        overlap_length += len(prev_sentence) + 1  # +1 for space
                    else:
                        break
                
                # Start new chunk with overlap
                current_chunk = overlap_sentences
                current_length = overlap_length
            
            # Add the current sentence to the chunk
            current_chunk.append(sentence)
            current_length += sentence_length + 1  # +1 for space
        
        # Add the last chunk if it has content
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def process_content(self, content: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Process content by chunking and attach metadata to each chunk.
        
        Args:
            content: Text content
            metadata: Metadata to attach to each chunk
            
        Returns:
            List of dictionaries containing chunk text and metadata
        """
        # Chunk the text
        chunks = self.chunk_text_by_sentences(content)
        
        # Prepare result with metadata
        result = []
        for i, chunk_text in enumerate(chunks):
            # Create a copy of metadata for this chunk
            chunk_metadata = metadata.copy()
            
            # Add chunk-specific metadata
            chunk_metadata['chunk_id'] = f"{metadata.get('source', 'doc')}_{i+1}"
            chunk_metadata['chunk_index'] = i
            chunk_metadata['total_chunks'] = len(chunks)
            
            # Add to result
            result.append({
                "chunk_text": chunk_text,
                "metadata": chunk_metadata
            })
        
        logger.info(f"Processed content into {len(chunks)} chunks")
        return result

class PineconeUploader:
    """Handles uploading content to Pinecone."""
    
    def __init__(self, api_key: str, environment: str = "us-east-1", 
                 index_name: str = None, namespace: str = "default",
                 embedding_model: str = "llama-text-embed-v2"):
        """
        Initialize the PineconeUploader.
        
        Args:
            api_key: Pinecone API key
            environment: Pinecone environment (e.g., us-east-1)
            index_name: Name of the Pinecone index
            namespace: Namespace for the vectors
            embedding_model: Name of the embedding model
        """
        self.api_key = api_key
        self.environment = environment
        self.index_name = index_name
        self.namespace = namespace
        self.embedding_model = embedding_model
        
        # Initialize Pinecone client
        self.pc = Pinecone(api_key=self.api_key)
        
        # Ensure index exists
        self._ensure_index_exists()
    
    def _ensure_index_exists(self) -> None:
        """Ensure that the Pinecone index exists, creating it if necessary."""
        # Check if index exists
        if not self.pc.has_index(self.index_name):
            logger.info(f"Creating new index: {self.index_name}")
            
            # Create index with integrated embedding model
            self.pc.create_index_for_model(
                name=self.index_name,
                cloud="aws",
                region=self.environment,
                embed={
                    "model": self.embedding_model,
                    "field_map": {"text": "chunk_text"}
                }
            )
            logger.info(f"Index {self.index_name} created successfully")
        else:
            logger.info(f"Using existing index: {self.index_name}")
    
    def upsert_records(self, records: List[Dict[str, Any]]) -> int:
        """
        Upsert records to the Pinecone index.
        
        Args:
            records: List of records to upsert
            
        Returns:
            Number of records upserted
        """
        # Get the index
        index = self.pc.index(self.index_name)
        
        # Upsert records
        logger.info(f"Upserting {len(records)} records to namespace '{self.namespace}'")
        index.upsert_records(records=records, namespace=self.namespace)
        
        # Wait a moment for consistency
        time.sleep(2)
        
        # Get index stats
        stats = index.describe_index_stats()
        logger.info(f"Index stats after upsert: {stats}")
        
        return len(records)

def read_markdown_file(file_path: str) -> str:
    """
    Read content from a markdown file.
    
    Args:
        file_path: Path to the markdown file
        
    Returns:
        Content of the markdown file
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def extract_metadata_from_filename(file_path: str, customer_id: str) -> Dict[str, Any]:
    """
    Extract metadata from the filename and file content.
    
    Args:
        file_path: Path to the markdown file
        customer_id: Customer identifier
        
    Returns:
        Dictionary containing metadata
    """
    # Extract filename without extension
    filename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(filename)[0]
    
    # Basic metadata
    metadata = {
        "source": name_without_ext,
        "file_path": file_path,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "customer_id": customer_id
    }
    
    # Try to extract title from content
    content = read_markdown_file(file_path)
    title_match = None
    
    # Look for # Title format
    title_pattern = re.compile(r'^#\s+(.+)$', re.MULTILINE)
    title_match = title_pattern.search(content)
    
    if title_match:
        metadata["title"] = title_match.group(1).strip()
    else:
        # Use filename as title if no title found in content
        metadata["title"] = name_without_ext.replace('_', ' ').title()
    
    return metadata

def save_chunks_to_disk(chunks: List[Dict[str, Any]], output_dir: str) -> List[str]:
    """
    Save chunks to disk for dry run mode.
    
    Args:
        chunks: List of dictionaries containing chunk text and metadata
        output_dir: Directory to save chunks
        
    Returns:
        List of saved file paths
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    saved_files = []
    for i, chunk in enumerate(chunks):
        # Generate filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chunk_id = chunk["metadata"].get("chunk_id", f"chunk_{i}")
        filename = f"{output_dir}/{timestamp}_{chunk_id}.json"
        
        # Prepare data to save
        import json
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                "chunk_text": chunk["chunk_text"],
                "metadata": chunk["metadata"]
            }, f, ensure_ascii=False, indent=2)
        
        saved_files.append(filename)
    
    logger.info(f"Saved {len(saved_files)} chunks to {output_dir}")
    return saved_files

def process_markdown_files(input_dir: str, dry_run: bool = False) -> int:
    """
    Process markdown files and upload to Pinecone.
    
    Args:
        input_dir: Directory containing markdown files
        dry_run: Whether to run in dry run mode
        
    Returns:
        Number of files processed
    """
    # Load environment variables
    load_dotenv()
    
    # Check required environment variables
    required_vars = ["PINECONE_API_KEY", "CUSTOMER_ID"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        logger.error(f"Missing required environment variables: {', '.join(missing_vars)}")
        return 0
    
    # Get environment variables
    api_key = os.getenv("PINECONE_API_KEY")
    customer_id = os.getenv("CUSTOMER_ID")
    environment = os.getenv("PINECONE_ENVIRONMENT", "us-east-1")
    index_name = os.getenv("PINECONE_INDEX_NAME", f"rag-{customer_id.lower()}")
    namespace = os.getenv("NAMESPACE", "default")
    embedding_model = os.getenv("EMBEDDING_MODEL", "llama-text-embed-v2")
    chunk_size = int(os.getenv("CHUNK_SIZE", "500"))
    chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "50"))
    
    # Initialize content chunker
    chunker = ContentChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    
    # Check if directory exists
    if not os.path.exists(input_dir):
        logger.error(f"Directory not found: {input_dir}")
        return 0
    
    # Get all markdown files
    markdown_files = glob.glob(os.path.join(input_dir, "*.md"))
    
    if not markdown_files:
        logger.warning(f"No markdown files found in {input_dir}")
        return 0
    
    logger.info(f"Found {len(markdown_files)} markdown files in {input_dir}")
    
    # Process each file
    all_chunks = []
    for file_path in markdown_files:
        try:
            # Read content
            content = read_markdown_file(file_path)
            
            # Extract metadata
            metadata = extract_metadata_from_filename(file_path, customer_id)
            
            # Process content into chunks
            chunks = chunker.process_content(content, metadata)
            all_chunks.extend(chunks)
            
            logger.info(f"Processed {file_path} into {len(chunks)} chunks")
        except Exception as e:
            logger.error(f"Error processing {file_path}: {str(e)}")
            continue
    
    # Handle chunks based on mode
    if dry_run:
        # Dry run mode: save to disk
        output_dir = os.getenv("OUTPUT_DIR", "processed_content")
        save_chunks_to_disk(all_chunks, output_dir)
    else:
        # Embedding mode: upload to Pinecone
        try:
            # Initialize Pinecone uploader
            uploader = PineconeUploader(
                api_key=api_key,
                environment=environment,
                index_name=index_name,
                namespace=namespace,
                embedding_model=embedding_model
            )
            
            # Convert chunks to Pinecone records format
            records = []
            for chunk in all_chunks:
                record = {
                    "_id": chunk["metadata"].get("chunk_id", str(uuid.uuid4())),
                    "chunk_text": chunk["chunk_text"]
                }
                # Add all metadata as fields
                for key, value in chunk["metadata"].items():
                    record[key] = value
                
                records.append(record)
            
            # Upsert to Pinecone
            uploader.upsert_records(records)
        except Exception as e:
            logger.error(f"Error uploading to Pinecone: {str(e)}")
            return 0
    
    return len(markdown_files)

def main():
    """Main entry point for the script."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Upload markdown files to Pinecone'
    )
    parser.add_argument(
        '--dry', 
        action='store_true', 
        help='Run in dry mode (process but don\'t upload)'
    )
    parser.add_argument(
        '--input-dir', 
        type=str, 
        default='winery_content',
        help='Directory containing markdown files'
    )
    args = parser.parse_args()
    
    try:
        # Process markdown files
        num_processed = process_markdown_files(
            input_dir=args.input_dir,
            dry_run=args.dry
        )
        
        if num_processed > 0:
            logger.info(f"Successfully processed {num_processed} files")
            return 0
        else:
            logger.error("No files were processed")
            return 1
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
